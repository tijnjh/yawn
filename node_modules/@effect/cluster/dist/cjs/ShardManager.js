"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.makeClientRpc = exports.makeClientLocal = exports.make = exports.layerServerHandlers = exports.layerServer = exports.layerConfigFromEnv = exports.layerConfig = exports.layerClientRpc = exports.layerClientLocal = exports.layer = exports.configFromEnv = exports.configConfig = exports.ShardingEventSchema = exports.ShardingEvent = exports.ShardManagerClient = exports.ShardManager = exports.Rpcs = exports.Config = void 0;
var Rpc = _interopRequireWildcard(require("@effect/rpc/Rpc"));
var RpcClient = _interopRequireWildcard(require("@effect/rpc/RpcClient"));
var RpcGroup = _interopRequireWildcard(require("@effect/rpc/RpcGroup"));
var RpcServer = _interopRequireWildcard(require("@effect/rpc/RpcServer"));
var Arr = _interopRequireWildcard(require("effect/Array"));
var Clock = _interopRequireWildcard(require("effect/Clock"));
var Config_ = _interopRequireWildcard(require("effect/Config"));
var ConfigProvider = _interopRequireWildcard(require("effect/ConfigProvider"));
var Context = _interopRequireWildcard(require("effect/Context"));
var Data = _interopRequireWildcard(require("effect/Data"));
var Deferred = _interopRequireWildcard(require("effect/Deferred"));
var Duration = _interopRequireWildcard(require("effect/Duration"));
var Effect = _interopRequireWildcard(require("effect/Effect"));
var Equal = _interopRequireWildcard(require("effect/Equal"));
var FiberSet = _interopRequireWildcard(require("effect/FiberSet"));
var _Function = require("effect/Function");
var Iterable = _interopRequireWildcard(require("effect/Iterable"));
var Layer = _interopRequireWildcard(require("effect/Layer"));
var Mailbox = _interopRequireWildcard(require("effect/Mailbox"));
var Metric = _interopRequireWildcard(require("effect/Metric"));
var MetricLabel = _interopRequireWildcard(require("effect/MetricLabel"));
var MutableHashMap = _interopRequireWildcard(require("effect/MutableHashMap"));
var MutableHashSet = _interopRequireWildcard(require("effect/MutableHashSet"));
var Option = _interopRequireWildcard(require("effect/Option"));
var PubSub = _interopRequireWildcard(require("effect/PubSub"));
var Queue = _interopRequireWildcard(require("effect/Queue"));
var Schedule = _interopRequireWildcard(require("effect/Schedule"));
var Schema = _interopRequireWildcard(require("effect/Schema"));
var _ClusterError = require("./ClusterError.js");
var ClusterMetrics = _interopRequireWildcard(require("./ClusterMetrics.js"));
var _shardManager = require("./internal/shardManager.js");
var MachineId = _interopRequireWildcard(require("./MachineId.js"));
var _Runner = require("./Runner.js");
var _RunnerAddress = require("./RunnerAddress.js");
var _RunnerHealth = require("./RunnerHealth.js");
var _Runners = require("./Runners.js");
var _ShardId = require("./ShardId.js");
var _ShardingConfig = require("./ShardingConfig.js");
var _ShardStorage = require("./ShardStorage.js");
function _interopRequireWildcard(e, t) { if ("function" == typeof WeakMap) var r = new WeakMap(), n = new WeakMap(); return (_interopRequireWildcard = function (e, t) { if (!t && e && e.__esModule) return e; var o, i, f = { __proto__: null, default: e }; if (null === e || "object" != typeof e && "function" != typeof e) return f; if (o = t ? n : r) { if (o.has(e)) return o.get(e); o.set(e, f); } for (const t in e) "default" !== t && {}.hasOwnProperty.call(e, t) && ((i = (o = Object.defineProperty) && Object.getOwnPropertyDescriptor(e, t)) && (i.get || i.set) ? o(f, t, i) : f[t] = e[t]); return f; })(e, t); }
/**
 * @since 1.0.0
 */

/**
 * @since 1.0.0
 * @category models
 */
class ShardManager extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager")() {}
/**
 * @since 1.0.0
 * @category Config
 */
exports.ShardManager = ShardManager;
class Config extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager/Config")() {
  /**
   * @since 1.0.0
   */
  static defaults = {
    rebalanceDebounce: /*#__PURE__*/Duration.seconds(3),
    rebalanceInterval: /*#__PURE__*/Duration.seconds(20),
    rebalanceRetryInterval: /*#__PURE__*/Duration.seconds(10),
    rebalanceRate: 2 / 100,
    persistRetryCount: 100,
    persistRetryInterval: /*#__PURE__*/Duration.seconds(3),
    runnerHealthCheckInterval: /*#__PURE__*/Duration.minutes(1),
    runnerPingTimeout: /*#__PURE__*/Duration.seconds(3)
  };
}
/**
 * @since 1.0.0
 * @category Config
 */
exports.Config = Config;
const configConfig = exports.configConfig = /*#__PURE__*/Config_.all({
  rebalanceDebounce: /*#__PURE__*/Config_.duration("rebalanceDebounce").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceDebounce), /*#__PURE__*/Config_.withDescription("The duration to wait before rebalancing shards after a change.")),
  rebalanceInterval: /*#__PURE__*/Config_.duration("rebalanceInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceInterval), /*#__PURE__*/Config_.withDescription("The interval on which regular rebalancing of shards will occur.")),
  rebalanceRetryInterval: /*#__PURE__*/Config_.duration("rebalanceRetryInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceRetryInterval), /*#__PURE__*/Config_.withDescription("The interval on which rebalancing of shards which failed to be rebalanced will be retried.")),
  rebalanceRate: /*#__PURE__*/Config_.number("rebalanceRate").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceRate), /*#__PURE__*/Config_.withDescription("The maximum ratio of shards to rebalance at once.")),
  persistRetryCount: /*#__PURE__*/Config_.integer("persistRetryCount").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.persistRetryCount), /*#__PURE__*/Config_.withDescription("The number of times persistence of runners will be retried if it fails.")),
  persistRetryInterval: /*#__PURE__*/Config_.duration("persistRetryInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.persistRetryInterval), /*#__PURE__*/Config_.withDescription("The interval on which persistence of runners will be retried if it fails.")),
  runnerHealthCheckInterval: /*#__PURE__*/Config_.duration("runnerHealthCheckInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.runnerHealthCheckInterval), /*#__PURE__*/Config_.withDescription("The interval on which runner health will be checked.")),
  runnerPingTimeout: /*#__PURE__*/Config_.duration("runnerPingTimeout").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.runnerPingTimeout), /*#__PURE__*/Config_.withDescription("The length of time to wait for a runner to respond to a ping."))
});
/**
 * @since 1.0.0
 * @category Config
 */
const configFromEnv = exports.configFromEnv = /*#__PURE__*/configConfig.pipe(/*#__PURE__*/Effect.withConfigProvider(/*#__PURE__*/ConfigProvider.fromEnv().pipe(ConfigProvider.constantCase)));
/**
 * @since 1.0.0
 * @category Config
 */
const layerConfig = config => Layer.succeed(Config, {
  ...Config.defaults,
  ...config
});
/**
 * @since 1.0.0
 * @category Config
 */
exports.layerConfig = layerConfig;
const layerConfigFromEnv = config => Layer.effect(Config, config ? Effect.map(configFromEnv, env => ({
  ...env,
  ...config
})) : configFromEnv);
/**
 * Represents a client which can be used to communicate with the
 * `ShardManager`.
 *
 * @since 1.0.0
 * @category Client
 */
exports.layerConfigFromEnv = layerConfigFromEnv;
class ShardManagerClient extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager/ShardManagerClient")() {}
/**
 * @since 1.0.0
 * @category models
 */
exports.ShardManagerClient = ShardManagerClient;
const ShardingEventSchema = exports.ShardingEventSchema = /*#__PURE__*/Schema.Union(/*#__PURE__*/Schema.TaggedStruct("StreamStarted", {}), /*#__PURE__*/Schema.TaggedStruct("ShardsAssigned", {
  address: _RunnerAddress.RunnerAddress,
  shards: /*#__PURE__*/Schema.Array(_ShardId.ShardId)
}), /*#__PURE__*/Schema.TaggedStruct("ShardsUnassigned", {
  address: _RunnerAddress.RunnerAddress,
  shards: /*#__PURE__*/Schema.Array(_ShardId.ShardId)
}), /*#__PURE__*/Schema.TaggedStruct("RunnerRegistered", {
  address: _RunnerAddress.RunnerAddress
}), /*#__PURE__*/Schema.TaggedStruct("RunnerUnregistered", {
  address: _RunnerAddress.RunnerAddress
}));
/**
 * The messaging protocol for the `ShardManager`.
 *
 * @since 1.0.0
 * @category Rpcs
 */
class Rpcs extends /*#__PURE__*/RpcGroup.make(/*#__PURE__*/Rpc.make("Register", {
  payload: {
    runner: _Runner.Runner
  },
  success: MachineId.MachineId
}), /*#__PURE__*/Rpc.make("Unregister", {
  payload: {
    address: _RunnerAddress.RunnerAddress
  }
}), /*#__PURE__*/Rpc.make("NotifyUnhealthyRunner", {
  payload: {
    address: _RunnerAddress.RunnerAddress
  }
}), /*#__PURE__*/Rpc.make("GetAssignments", {
  success: /*#__PURE__*/Schema.Array(/*#__PURE__*/Schema.Tuple(_ShardId.ShardId, /*#__PURE__*/Schema.Option(_RunnerAddress.RunnerAddress)))
}), /*#__PURE__*/Rpc.make("ShardingEvents", {
  payload: {
    address: /*#__PURE__*/Schema.Option(_RunnerAddress.RunnerAddress)
  },
  success: ShardingEventSchema,
  error: _ClusterError.RunnerNotRegistered,
  stream: true
}), /*#__PURE__*/Rpc.make("GetTime", {
  success: Schema.Number
})) {}
/**
 * @since 1.0.0
 * @category models
 */
exports.Rpcs = Rpcs;
const ShardingEvent = exports.ShardingEvent = /*#__PURE__*/Data.taggedEnum();
/**
 * @since 1.0.0
 * @category Client
 */
const makeClientLocal = exports.makeClientLocal = /*#__PURE__*/Effect.gen(function* () {
  const config = yield* _ShardingConfig.ShardingConfig;
  const clock = yield* Effect.clock;
  const groups = new Set();
  const shards = MutableHashMap.empty();
  let machineId = 0;
  return ShardManagerClient.of({
    register: (_, groupsToAdd) => Effect.sync(() => {
      for (const group of groupsToAdd) {
        if (groups.has(group)) continue;
        groups.add(group);
        for (let n = 1; n <= config.shardsPerGroup; n++) {
          MutableHashMap.set(shards, (0, _ShardId.make)(group, n), config.runnerAddress);
        }
      }
      return MachineId.make(++machineId);
    }),
    unregister: () => Effect.void,
    notifyUnhealthyRunner: () => Effect.void,
    getAssignments: Effect.succeed(shards),
    shardingEvents: Effect.fnUntraced(function* (_address) {
      const mailbox = yield* Mailbox.make();
      yield* mailbox.offer(ShardingEvent.StreamStarted());
      return mailbox;
    }),
    getTime: clock.currentTimeMillis
  });
});
/**
 * @since 1.0.0
 * @category Client
 */
const makeClientRpc = exports.makeClientRpc = /*#__PURE__*/Effect.gen(function* () {
  const config = yield* _ShardingConfig.ShardingConfig;
  const client = yield* RpcClient.make(Rpcs, {
    spanPrefix: "ShardManagerClient",
    disableTracing: true
  });
  return ShardManagerClient.of({
    register: (address, groups) => client.Register({
      runner: _Runner.Runner.make({
        address,
        version: config.serverVersion,
        groups
      })
    }).pipe(Effect.orDie),
    unregister: address => Effect.orDie(client.Unregister({
      address
    })),
    notifyUnhealthyRunner: address => Effect.orDie(client.NotifyUnhealthyRunner({
      address
    })),
    getAssignments: Effect.orDie(client.GetAssignments()),
    shardingEvents: address => Mailbox.make().pipe(Effect.tap(Effect.fnUntraced(function* (mailbox) {
      const events = yield* client.ShardingEvents({
        address
      }, {
        asMailbox: true
      });
      const take = Effect.orDie(events.takeAll);
      while (true) {
        mailbox.unsafeOfferAll((yield* take)[0]);
      }
    }, (effect, mb) => Mailbox.into(effect, mb), Effect.forkScoped))),
    getTime: Effect.orDie(client.GetTime())
  });
});
/**
 * @since 1.0.0
 * @category Client
 */
const layerClientLocal = exports.layerClientLocal = /*#__PURE__*/Layer.effect(ShardManagerClient, makeClientLocal);
/**
 * @since 1.0.0
 * @category Client
 */
const layerClientRpc = exports.layerClientRpc = /*#__PURE__*/Layer.scoped(ShardManagerClient, makeClientRpc).pipe(/*#__PURE__*/Layer.provide(/*#__PURE__*/Layer.scoped(RpcClient.Protocol, /*#__PURE__*/Effect.gen(function* () {
  const config = yield* _ShardingConfig.ShardingConfig;
  const clientProtocol = yield* _Runners.RpcClientProtocol;
  return yield* clientProtocol(config.shardManagerAddress);
}))));
/**
 * @since 1.0.0
 * @category Constructors
 */
const make = exports.make = /*#__PURE__*/Effect.gen(function* () {
  const storage = yield* _ShardStorage.ShardStorage;
  const runnersApi = yield* _Runners.Runners;
  const runnerHealthApi = yield* _RunnerHealth.RunnerHealth;
  const clock = yield* Effect.clock;
  const config = yield* Config;
  const shardingConfig = yield* _ShardingConfig.ShardingConfig;
  const state = yield* Effect.orDie(_shardManager.State.fromStorage(shardingConfig.shardsPerGroup));
  const scope = yield* Effect.scope;
  const events = yield* PubSub.unbounded();
  function updateRunnerMetrics() {
    ClusterMetrics.runners.unsafeUpdate(MutableHashMap.size(state.allRunners), []);
  }
  function updateShardMetrics() {
    const stats = state.shardStats;
    for (const [address, shardCount] of stats.perRunner) {
      ClusterMetrics.assignedShards.unsafeUpdate(shardCount, [MetricLabel.make("address", address)]);
    }
    ClusterMetrics.unassignedShards.unsafeUpdate(stats.unassigned, []);
  }
  updateShardMetrics();
  function withRetry(effect) {
    return effect.pipe(Effect.retry({
      schedule: Schedule.spaced(config.persistRetryCount),
      times: config.persistRetryCount
    }), Effect.ignore);
  }
  const persistRunners = Effect.unsafeMakeSemaphore(1).withPermits(1)(withRetry(Effect.suspend(() => storage.saveRunners(Iterable.map(state.allRunners, ([address, runner]) => [address, runner.runner])))));
  const persistAssignments = Effect.unsafeMakeSemaphore(1).withPermits(1)(withRetry(Effect.suspend(() => storage.saveAssignments(state.assignments))));
  const notifyUnhealthyRunner = Effect.fnUntraced(function* (address) {
    if (!MutableHashMap.has(state.allRunners, address)) return;
    if (!(yield* runnerHealthApi.isAlive(address))) {
      yield* Effect.logWarning(`Runner at address '${address.toString()}' is not alive`);
      yield* unregister(address);
    }
  });
  function updateShardsState(shards, address) {
    return Effect.suspend(() => {
      if (Option.isSome(address) && !MutableHashMap.has(state.allRunners, address.value)) {
        return Effect.fail(new _ClusterError.RunnerNotRegistered({
          address: address.value
        }));
      }
      state.addAssignments(shards, address);
      return Effect.void;
    });
  }
  const getAssignments = Effect.sync(() => state.assignments);
  let machineId = 0;
  const register = Effect.fnUntraced(function* (runner) {
    yield* Effect.logInfo(`Registering runner ${_Runner.Runner.pretty(runner)}`);
    const current = MutableHashMap.get(state.allRunners, runner.address).pipe(Option.filter(r => r.runner.version === runner.version));
    if (Option.isSome(current)) {
      return MachineId.make(++machineId);
    }
    state.addRunner(runner, clock.unsafeCurrentTimeMillis());
    updateRunnerMetrics();
    yield* PubSub.publish(events, ShardingEvent.RunnerRegistered({
      address: runner.address
    }));
    yield* Effect.forkIn(persistRunners, scope);
    yield* Effect.forkIn(rebalance, scope);
    return MachineId.make(++machineId);
  });
  const unregister = Effect.fnUntraced(function* (address) {
    if (!MutableHashMap.has(state.allRunners, address)) return;
    yield* Effect.logInfo("Unregistering runner at address:", address);
    const unassignments = Arr.empty();
    for (const [shard, runner] of state.assignments) {
      if (Option.isSome(runner) && Equal.equals(runner.value, address)) {
        unassignments.push(shard);
      }
    }
    state.addAssignments(unassignments, Option.none());
    state.removeRunner(address);
    updateRunnerMetrics();
    if (unassignments.length > 0) {
      yield* PubSub.publish(events, ShardingEvent.RunnerUnregistered({
        address
      }));
    }
    yield* Effect.forkIn(persistRunners, scope);
    yield* Effect.forkIn(rebalance, scope);
  });
  let rebalancing = false;
  let rebalanceDeferred;
  const rebalanceFibers = yield* FiberSet.make();
  const rebalance = Effect.withFiberRuntime(fiber => {
    if (!rebalancing) {
      rebalancing = true;
      return rebalanceLoop;
    }
    if (!rebalanceDeferred) {
      rebalanceDeferred = Deferred.unsafeMake(fiber.id());
    }
    return Deferred.await(rebalanceDeferred);
  });
  const rebalanceLoop = Effect.suspend(() => {
    const deferred = rebalanceDeferred;
    rebalanceDeferred = undefined;
    return runRebalance.pipe(deferred ? Effect.intoDeferred(deferred) : _Function.identity, Effect.onExit(() => {
      if (!rebalanceDeferred) {
        rebalancing = false;
        return Effect.void;
      }
      return Effect.forkIn(rebalanceLoop, scope);
    }));
  });
  const runRebalance = Effect.gen(function* () {
    yield* Effect.sleep(config.rebalanceDebounce);
    if (state.shards.size === 0) {
      yield* Effect.logDebug("No shards to rebalance");
      return;
    }
    // Determine which shards to assign and unassign
    const assignments = MutableHashMap.empty();
    const unassignments = MutableHashMap.empty();
    const changes = MutableHashSet.empty();
    for (const group of state.shards.keys()) {
      const [groupAssignments, groupUnassignments, groupChanges] = (0, _shardManager.decideAssignmentsForShards)(state, group);
      for (const [address, shards] of groupAssignments) {
        (0, _shardManager.addAllNested)(assignments, address, Array.from(shards, id => (0, _ShardId.make)(group, id)));
      }
      for (const [address, shards] of groupUnassignments) {
        (0, _shardManager.addAllNested)(unassignments, address, Array.from(shards, id => (0, _ShardId.make)(group, id)));
      }
      for (const address of groupChanges) {
        MutableHashSet.add(changes, address);
      }
    }
    yield* Effect.logDebug(`Rebalancing shards`);
    if (MutableHashSet.size(changes) === 0) return;
    yield* Metric.increment(ClusterMetrics.rebalances);
    // Ping runners first and remove unhealthy ones
    const failedRunners = MutableHashSet.empty();
    for (const address of changes) {
      yield* FiberSet.run(rebalanceFibers, runnersApi.ping(address).pipe(Effect.timeout(config.runnerPingTimeout), Effect.catchAll(() => {
        MutableHashSet.add(failedRunners, address);
        MutableHashMap.remove(assignments, address);
        MutableHashMap.remove(unassignments, address);
        return Effect.void;
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    const failedUnassignments = new Set();
    for (const [address, shards] of unassignments) {
      yield* FiberSet.run(rebalanceFibers, updateShardsState(shards, Option.none()).pipe(Effect.matchEffect({
        onFailure: () => {
          MutableHashSet.add(failedRunners, address);
          for (const shard of shards) {
            failedUnassignments.add(shard);
          }
          // Remove failed runners from the assignments
          MutableHashMap.remove(assignments, address);
          return Effect.void;
        },
        onSuccess: () => PubSub.publish(events, ShardingEvent.ShardsUnassigned({
          address,
          shards: Array.from(shards)
        }))
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    // Remove failed shard unassignments from the assignments
    MutableHashMap.forEach(assignments, (shards, address) => {
      for (const shard of failedUnassignments) {
        MutableHashSet.remove(shards, shard);
      }
      if (MutableHashSet.size(shards) === 0) {
        MutableHashMap.remove(assignments, address);
      }
    });
    // Perform the assignments
    for (const [address, shards] of assignments) {
      yield* FiberSet.run(rebalanceFibers, updateShardsState(shards, Option.some(address)).pipe(Effect.matchEffect({
        onFailure: () => {
          MutableHashSet.add(failedRunners, address);
          return Effect.void;
        },
        onSuccess: () => PubSub.publish(events, ShardingEvent.ShardsAssigned({
          address,
          shards: Array.from(shards)
        }))
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    updateShardMetrics();
    const wereFailures = MutableHashSet.size(failedRunners) > 0;
    if (wereFailures) {
      // Check if the failing runners are still reachable
      yield* Effect.forEach(failedRunners, notifyUnhealthyRunner, {
        discard: true
      }).pipe(Effect.forkIn(scope));
      yield* Effect.logWarning("Failed to rebalance runners: ", failedRunners);
    }
    if (wereFailures) {
      // Try rebalancing again later if there were any failures
      yield* Clock.sleep(config.rebalanceRetryInterval).pipe(Effect.zipRight(rebalance), Effect.forkIn(scope));
    }
    yield* persistAssignments;
  }).pipe(Effect.withSpan("ShardManager.rebalance", {
    captureStackTrace: false
  }));
  const checkRunnerHealth = Effect.suspend(() => Effect.forEach(MutableHashMap.keys(state.allRunners), notifyUnhealthyRunner, {
    concurrency: 10,
    discard: true
  }));
  yield* Effect.addFinalizer(() => persistAssignments.pipe(Effect.catchAllCause(cause => Effect.logWarning("Failed to persist assignments on shutdown", cause)), Effect.zipRight(persistRunners.pipe(Effect.catchAllCause(cause => Effect.logWarning("Failed to persist runners on shutdown", cause))))));
  yield* Effect.forkIn(persistRunners, scope);
  // Start a regular cluster rebalance at the configured interval
  yield* rebalance.pipe(Effect.andThen(Effect.sleep(config.rebalanceInterval)), Effect.forever, Effect.forkIn(scope));
  yield* checkRunnerHealth.pipe(Effect.andThen(Effect.sleep(config.runnerHealthCheckInterval)), Effect.forever, Effect.forkIn(scope));
  yield* Effect.gen(function* () {
    const queue = yield* PubSub.subscribe(events);
    while (true) {
      yield* Effect.logInfo("Shard manager event:", yield* Queue.take(queue));
    }
  }).pipe(Effect.forkIn(scope));
  yield* Effect.logInfo("Shard manager initialized");
  return ShardManager.of({
    getAssignments,
    shardingEvents: address => {
      if (Option.isNone(address)) {
        return PubSub.subscribe(events);
      }
      return Effect.tap(PubSub.subscribe(events), () => {
        const isRegistered = MutableHashMap.has(state.allRunners, address.value);
        if (isRegistered) {
          return runnerHealthApi.onConnection(address.value);
        }
        return Effect.fail(new _ClusterError.RunnerNotRegistered({
          address: address.value
        }));
      });
    },
    register,
    unregister,
    rebalance,
    notifyUnhealthyRunner,
    checkRunnerHealth
  });
});
/**
 * @since 1.0.0
 * @category layer
 */
const layer = exports.layer = /*#__PURE__*/Layer.scoped(ShardManager, make);
/**
 * @since 1.0.0
 * @category Server
 */
const layerServerHandlers = exports.layerServerHandlers = /*#__PURE__*/Rpcs.toLayer(/*#__PURE__*/Effect.gen(function* () {
  const shardManager = yield* ShardManager;
  const clock = yield* Effect.clock;
  return {
    Register: ({
      runner
    }) => shardManager.register(runner),
    Unregister: ({
      address
    }) => shardManager.unregister(address),
    NotifyUnhealthyRunner: ({
      address
    }) => shardManager.notifyUnhealthyRunner(address),
    GetAssignments: () => Effect.map(shardManager.getAssignments, assignments => Array.from(assignments)),
    ShardingEvents: Effect.fnUntraced(function* ({
      address
    }) {
      const queue = yield* shardManager.shardingEvents(address);
      const mailbox = yield* Mailbox.make();
      yield* mailbox.offer(ShardingEvent.StreamStarted());
      yield* Queue.takeBetween(queue, 1, Number.MAX_SAFE_INTEGER).pipe(Effect.flatMap(events => mailbox.offerAll(events)), Effect.forever, Effect.forkScoped);
      return mailbox;
    }),
    GetTime: () => clock.currentTimeMillis
  };
}));
/**
 * @since 1.0.0
 * @category Server
 */
const layerServer = exports.layerServer = /*#__PURE__*/RpcServer.layer(Rpcs, {
  spanPrefix: "ShardManager",
  disableTracing: true
}).pipe(/*#__PURE__*/Layer.provide(layerServerHandlers));
//# sourceMappingURL=ShardManager.js.map