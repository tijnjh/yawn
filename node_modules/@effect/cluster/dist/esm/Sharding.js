import * as RpcClient from "@effect/rpc/RpcClient";
import { RequestId } from "@effect/rpc/RpcMessage";
import * as Arr from "effect/Array";
import * as Cause from "effect/Cause";
import * as Context from "effect/Context";
import * as Deferred from "effect/Deferred";
import * as Effect from "effect/Effect";
import * as Equal from "effect/Equal";
import * as Exit from "effect/Exit";
import * as Fiber from "effect/Fiber";
import * as FiberHandle from "effect/FiberHandle";
import * as FiberMap from "effect/FiberMap";
import * as FiberRef from "effect/FiberRef";
import { constant } from "effect/Function";
import * as HashMap from "effect/HashMap";
import * as Iterable from "effect/Iterable";
import * as Layer from "effect/Layer";
import * as MutableHashMap from "effect/MutableHashMap";
import * as MutableHashSet from "effect/MutableHashSet";
import * as MutableRef from "effect/MutableRef";
import * as Option from "effect/Option";
import * as Predicate from "effect/Predicate";
import * as PubSub from "effect/PubSub";
import * as Schedule from "effect/Schedule";
import * as Scope from "effect/Scope";
import * as Stream from "effect/Stream";
import { AlreadyProcessingMessage, EntityNotAssignedToRunner, EntityNotManagedByRunner, RunnerUnavailable } from "./ClusterError.js";
import * as ClusterError from "./ClusterError.js";
import { Persisted, Uninterruptible } from "./ClusterSchema.js";
import * as ClusterSchema from "./ClusterSchema.js";
import { EntityAddress } from "./EntityAddress.js";
import { EntityId } from "./EntityId.js";
import * as Envelope from "./Envelope.js";
import * as EntityManager from "./internal/entityManager.js";
import { EntityReaper } from "./internal/entityReaper.js";
import { hashString } from "./internal/hash.js";
import { internalInterruptors } from "./internal/interruptors.js";
import { ResourceMap } from "./internal/resourceMap.js";
import * as Message from "./Message.js";
import * as MessageStorage from "./MessageStorage.js";
import * as Reply from "./Reply.js";
import { Runners } from "./Runners.js";
import { ShardId } from "./ShardId.js";
import { ShardingConfig } from "./ShardingConfig.js";
import { EntityRegistered, SingletonRegistered } from "./ShardingRegistrationEvent.js";
import { ShardManagerClient } from "./ShardManager.js";
import { ShardStorage } from "./ShardStorage.js";
import { SingletonAddress } from "./SingletonAddress.js";
import * as Snowflake from "./Snowflake.js";
/**
 * @since 1.0.0
 * @category models
 */
export class Sharding extends /*#__PURE__*/Context.Tag("@effect/cluster/Sharding")() {}
const make = /*#__PURE__*/Effect.gen(function* () {
  const config = yield* ShardingConfig;
  const runners = yield* Runners;
  const shardManager = yield* ShardManagerClient;
  const snowflakeGen = yield* Snowflake.Generator;
  const shardingScope = yield* Effect.scope;
  const isShutdown = MutableRef.make(false);
  const storage = yield* MessageStorage.MessageStorage;
  const storageEnabled = storage !== MessageStorage.noop;
  const shardStorage = yield* ShardStorage;
  const entityManagers = new Map();
  const shardAssignments = MutableHashMap.empty();
  const selfShards = MutableHashSet.empty();
  // the active shards are the ones that we have acquired the lock for
  const acquiredShards = MutableHashSet.empty();
  const activeShardsLatch = yield* Effect.makeLatch(false);
  const events = yield* PubSub.unbounded();
  const getRegistrationEvents = Stream.fromPubSub(events);
  const isLocalRunner = address => Option.isSome(config.runnerAddress) && Equal.equals(address, config.runnerAddress.value);
  function getShardId(entityId, group) {
    const id = Math.abs(hashString(entityId) % config.shardsPerGroup) + 1;
    return ShardId.make({
      group,
      id
    }, {
      disableValidation: true
    });
  }
  function isEntityOnLocalShards(address) {
    return MutableHashSet.has(acquiredShards, address.shardId);
  }
  // --- Shard acquisition ---
  if (Option.isSome(config.runnerAddress)) {
    const selfAddress = config.runnerAddress.value;
    yield* Scope.addFinalizerExit(shardingScope, () => {
      // the locks expire over time, so if this fails we ignore it
      return Effect.ignore(shardStorage.releaseAll(selfAddress));
    });
    const releasingShards = MutableHashSet.empty();
    yield* Effect.gen(function* () {
      activeShardsLatch.unsafeOpen();
      while (true) {
        yield* activeShardsLatch.await;
        activeShardsLatch.unsafeClose();
        // if a shard is no longer assigned to this runner, we release it
        for (const shardId of acquiredShards) {
          if (MutableHashSet.has(selfShards, shardId)) continue;
          MutableHashSet.remove(acquiredShards, shardId);
          MutableHashSet.add(releasingShards, shardId);
        }
        // if a shard has been assigned to this runner, we acquire it
        const unacquiredShards = MutableHashSet.empty();
        for (const shardId of selfShards) {
          if (MutableHashSet.has(acquiredShards, shardId) || MutableHashSet.has(releasingShards, shardId)) continue;
          MutableHashSet.add(unacquiredShards, shardId);
        }
        if (MutableHashSet.size(releasingShards) > 0) {
          yield* Effect.forkIn(syncSingletons, shardingScope);
          yield* releaseShards;
        }
        if (MutableHashSet.size(unacquiredShards) === 0) {
          continue;
        }
        const acquired = yield* shardStorage.acquire(selfAddress, unacquiredShards);
        yield* Effect.ignore(storage.resetShards(acquired));
        for (const shardId of acquired) {
          MutableHashSet.add(acquiredShards, shardId);
        }
        if (acquired.length > 0) {
          yield* storageReadLatch.open;
          yield* Effect.forkIn(syncSingletons, shardingScope);
        }
        yield* Effect.sleep(1000);
        activeShardsLatch.unsafeOpen();
      }
    }).pipe(Effect.catchAllCause(cause => Effect.logWarning("Could not acquire/release shards", cause)), Effect.forever, Effect.annotateLogs({
      package: "@effect/cluster",
      module: "Sharding",
      fiber: "Shard acquisition loop",
      runner: selfAddress
    }), Effect.interruptible, Effect.forkIn(shardingScope));
    // refresh the shard locks every 4s
    yield* Effect.suspend(() => shardStorage.refresh(selfAddress, [...acquiredShards, ...releasingShards])).pipe(Effect.flatMap(acquired => {
      for (const shardId of acquiredShards) {
        if (!acquired.some(_ => _[Equal.symbol](shardId))) {
          MutableHashSet.remove(acquiredShards, shardId);
          MutableHashSet.add(releasingShards, shardId);
        }
      }
      return MutableHashSet.size(releasingShards) > 0 ? Effect.andThen(Effect.forkIn(syncSingletons, shardingScope), releaseShards) : Effect.void;
    }), Effect.retry({
      times: 5,
      schedule: Schedule.spaced(50)
    }), Effect.catchAllCause(cause => Effect.logError("Could not refresh shard locks", cause).pipe(Effect.andThen(clearSelfShards))), Effect.schedule(Schedule.fixed(4000)), Effect.interruptible, Effect.forkIn(shardingScope));
    const releaseShardsLock = Effect.unsafeMakeSemaphore(1).withPermits(1);
    const releaseShards = releaseShardsLock(Effect.suspend(() => Effect.forEach(releasingShards, shardId => Effect.forEach(entityManagers.values(), state => state.manager.interruptShard(shardId), {
      concurrency: "unbounded",
      discard: true
    }).pipe(Effect.andThen(shardStorage.release(selfAddress, shardId)), Effect.annotateLogs({
      runner: selfAddress
    }), Effect.andThen(() => {
      MutableHashSet.remove(releasingShards, shardId);
    })), {
      concurrency: "unbounded",
      discard: true
    })).pipe(Effect.andThen(activeShardsLatch.open)));
  }
  const clearSelfShards = Effect.suspend(() => {
    MutableHashSet.clear(selfShards);
    return activeShardsLatch.open;
  });
  // --- Singletons ---
  const singletons = new Map();
  const singletonFibers = yield* FiberMap.make();
  const withSingletonLock = Effect.unsafeMakeSemaphore(1).withPermits(1);
  const registerSingleton = Effect.fnUntraced(function* (name, run, options) {
    const shardGroup = options?.shardGroup ?? "default";
    const address = new SingletonAddress({
      shardId: getShardId(EntityId.make(name), shardGroup),
      name
    });
    let map = singletons.get(address.shardId);
    if (!map) {
      map = MutableHashMap.empty();
      singletons.set(address.shardId, map);
    }
    if (MutableHashMap.has(map, address)) {
      return yield* Effect.dieMessage(`Singleton '${name}' is already registered`);
    }
    const context = yield* Effect.context();
    const wrappedRun = run.pipe(Effect.locally(FiberRef.currentLogAnnotations, HashMap.empty()), Effect.andThen(Effect.never), Effect.scoped, Effect.provide(context), Effect.orDie, Effect.interruptible);
    MutableHashMap.set(map, address, wrappedRun);
    yield* PubSub.publish(events, SingletonRegistered({
      address
    }));
    // start if we are on the right shard
    if (MutableHashSet.has(acquiredShards, address.shardId)) {
      yield* Effect.logDebug("Starting singleton", address);
      yield* FiberMap.run(singletonFibers, address, wrappedRun);
    }
  }, withSingletonLock);
  const syncSingletons = withSingletonLock(Effect.gen(function* () {
    for (const [shardId, map] of singletons) {
      for (const [address, run] of map) {
        const running = FiberMap.unsafeHas(singletonFibers, address);
        const shouldBeRunning = MutableHashSet.has(acquiredShards, shardId);
        if (running && !shouldBeRunning) {
          yield* Effect.logDebug("Stopping singleton", address);
          internalInterruptors.add(yield* Effect.fiberId);
          yield* FiberMap.remove(singletonFibers, address);
        } else if (!running && shouldBeRunning) {
          yield* Effect.logDebug("Starting singleton", address);
          yield* FiberMap.run(singletonFibers, address, run);
        }
      }
    }
  }));
  // --- Storage inbox ---
  const storageReadLatch = yield* Effect.makeLatch(true);
  const openStorageReadLatch = constant(storageReadLatch.open);
  const storageReadLock = Effect.unsafeMakeSemaphore(1);
  const withStorageReadLock = storageReadLock.withPermits(1);
  let storageAlreadyProcessed = _message => true;
  // keep track of the last sent request ids to avoid duplicates
  // we only keep the last 30 sets to avoid memory leaks
  const sentRequestIds = new Set();
  const sentRequestIdSets = new Set();
  if (storageEnabled && Option.isSome(config.runnerAddress)) {
    const selfAddress = config.runnerAddress.value;
    yield* Effect.gen(function* () {
      yield* Effect.logDebug("Starting");
      yield* Effect.addFinalizer(() => Effect.logDebug("Shutting down"));
      sentRequestIds.clear();
      sentRequestIdSets.clear();
      storageAlreadyProcessed = message => {
        if (!sentRequestIds.has(message.envelope.requestId)) {
          return false;
        }
        const state = entityManagers.get(message.envelope.address.entityType);
        if (!state) return true;
        return !state.manager.isProcessingFor(message, {
          excludeReplies: true
        });
      };
      while (true) {
        // wait for the next poll interval, or if we get notified of a change
        yield* storageReadLatch.await;
        // if we get notified of a change, ensure we start a read immediately
        // next iteration
        storageReadLatch.unsafeClose();
        // the lock is used to ensure resuming entities have a garantee that no
        // more items are added to the unprocessed set while the semaphore is
        // acquired.
        yield* storageReadLock.take(1);
        const messages = yield* storage.unprocessedMessages(acquiredShards);
        const currentSentRequestIds = new Set();
        sentRequestIdSets.add(currentSentRequestIds);
        const send = Effect.catchAllCause(Effect.suspend(() => {
          const message = messages[index];
          if (message._tag === "IncomingRequest") {
            if (sentRequestIds.has(message.envelope.requestId)) {
              return Effect.void;
            }
            sentRequestIds.add(message.envelope.requestId);
            currentSentRequestIds.add(message.envelope.requestId);
          }
          const address = message.envelope.address;
          if (!MutableHashSet.has(acquiredShards, address.shardId)) {
            return Effect.void;
          }
          const state = entityManagers.get(address.entityType);
          if (!state) {
            if (message._tag === "IncomingRequest") {
              return Effect.orDie(message.respond(Reply.ReplyWithContext.fromDefect({
                id: snowflakeGen.unsafeNext(),
                requestId: message.envelope.requestId,
                defect: new EntityNotManagedByRunner({
                  address
                })
              })));
            }
            return Effect.void;
          }
          const isProcessing = state.manager.isProcessingFor(message);
          // If the message might affect a currently processing request, we
          // send it to the entity manager to be processed.
          if (message._tag === "IncomingEnvelope" && isProcessing) {
            return state.manager.send(message);
          } else if (isProcessing) {
            return Effect.void;
          }
          // If the entity was resuming in another fiber, we add the message
          // id to the unprocessed set.
          const resumptionState = MutableHashMap.get(entityResumptionState, address);
          if (Option.isSome(resumptionState)) {
            resumptionState.value.unprocessed.add(message.envelope.requestId);
            if (message.envelope._tag === "Interrupt") {
              resumptionState.value.interrupts.set(message.envelope.requestId, message);
            }
            return Effect.void;
          }
          return state.manager.send(message);
        }), cause => {
          const message = messages[index];
          const error = Cause.failureOption(cause);
          // if we get a defect, then update storage
          if (Option.isNone(error)) {
            if (Cause.isInterrupted(cause)) {
              return Effect.void;
            }
            return storage.saveReply(Reply.ReplyWithContext.fromDefect({
              id: snowflakeGen.unsafeNext(),
              requestId: message.envelope.requestId,
              defect: Cause.squash(cause)
            }));
          }
          if (error.value._tag === "MailboxFull") {
            // MailboxFull can only happen for requests, so this cast is safe
            return resumeEntityFromStorage(message);
          }
          return Effect.void;
        });
        let index = 0;
        yield* Effect.whileLoop({
          while: () => index < messages.length,
          step: () => index++,
          body: constant(send)
        });
        // let the resuming entities check if they are done
        yield* storageReadLock.release(1);
        while (sentRequestIdSets.size > 30) {
          const oldest = Iterable.unsafeHead(sentRequestIdSets);
          sentRequestIdSets.delete(oldest);
          for (const id of oldest) {
            sentRequestIds.delete(id);
          }
        }
      }
    }).pipe(Effect.scoped, Effect.ensuring(storageReadLock.releaseAll), Effect.catchAllCause(cause => Effect.logWarning("Could not read messages from storage", cause)), Effect.repeat(Schedule.spaced(config.entityMessagePollInterval)), Effect.annotateLogs({
      package: "@effect/cluster",
      module: "Sharding",
      fiber: "Storage read loop",
      runner: selfAddress
    }), Effect.interruptible, Effect.forkIn(shardingScope));
    // open the storage latch every poll interval
    yield* storageReadLatch.open.pipe(Effect.delay(config.entityMessagePollInterval), Effect.forever, Effect.interruptible, Effect.forkIn(shardingScope));
    // Resume unprocessed messages for entities that reached a full mailbox.
    const entityResumptionState = MutableHashMap.empty();
    const resumeEntityFromStorage = lastReceivedMessage => {
      const address = lastReceivedMessage.envelope.address;
      const resumptionState = MutableHashMap.get(entityResumptionState, address);
      if (Option.isSome(resumptionState)) {
        resumptionState.value.unprocessed.add(lastReceivedMessage.envelope.requestId);
        return Effect.void;
      }
      MutableHashMap.set(entityResumptionState, address, {
        unprocessed: new Set([lastReceivedMessage.envelope.requestId]),
        interrupts: new Map()
      });
      return resumeEntityFromStorageImpl(address);
    };
    const resumeEntityFromStorageImpl = Effect.fnUntraced(function* (address) {
      const state = entityManagers.get(address.entityType);
      if (!state) {
        MutableHashMap.remove(entityResumptionState, address);
        return;
      }
      const resumptionState = Option.getOrThrow(MutableHashMap.get(entityResumptionState, address));
      let done = false;
      while (!done) {
        // if the shard is no longer assigned to this runner, we stop
        if (!MutableHashSet.has(acquiredShards, address.shardId)) {
          return;
        }
        // take a batch of unprocessed messages ids
        const messageIds = Arr.empty();
        for (const id of resumptionState.unprocessed) {
          if (messageIds.length === 1024) break;
          messageIds.push(id);
        }
        const messages = yield* storage.unprocessedMessagesById(messageIds);
        // this should not happen, but we handle it just in case
        if (messages.length === 0) {
          yield* Effect.sleep(config.entityMessagePollInterval);
          continue;
        }
        let index = 0;
        const sendWithRetry = Effect.catchTags(Effect.suspend(() => {
          if (!MutableHashSet.has(acquiredShards, address.shardId)) {
            return Effect.fail(new EntityNotAssignedToRunner({
              address
            }));
          }
          const message = messages[index];
          // check if this is a request that was interrupted
          const interrupt = message._tag === "IncomingRequest" && resumptionState.interrupts.get(message.envelope.requestId);
          return interrupt ? Effect.flatMap(state.manager.send(message), () => {
            resumptionState.interrupts.delete(message.envelope.requestId);
            return state.manager.send(interrupt);
          }) : state.manager.send(message);
        }), {
          MailboxFull: () => Effect.delay(sendWithRetry, config.sendRetryInterval),
          AlreadyProcessingMessage: () => Effect.void
        });
        yield* Effect.whileLoop({
          while: () => index < messages.length,
          body: constant(sendWithRetry),
          step: () => index++
        });
        for (const id of messageIds) {
          resumptionState.unprocessed.delete(id);
        }
        if (resumptionState.unprocessed.size > 0) continue;
        // if we have caught up to the main storage loop, we let it take over
        yield* withStorageReadLock(Effect.sync(() => {
          if (resumptionState.unprocessed.size === 0) {
            MutableHashMap.remove(entityResumptionState, address);
            done = true;
          }
        }));
      }
    }, Effect.retry({
      while: e => e._tag === "PersistenceError",
      schedule: Schedule.spaced(config.entityMessagePollInterval)
    }), Effect.catchAllCause(cause => Effect.logError("Could not resume unprocessed messages", cause)), (effect, address) => Effect.annotateLogs(effect, {
      package: "@effect/cluster",
      module: "Sharding",
      fiber: "Resuming unprocessed messages",
      runner: selfAddress,
      entity: address
    }), (effect, address) => Effect.ensuring(effect, Effect.sync(() => MutableHashMap.remove(entityResumptionState, address))), Effect.interruptible, Effect.forkIn(shardingScope));
  }
  // --- Sending messages ---
  const sendLocal = message => Effect.suspend(() => {
    const address = message.envelope.address;
    if (!isEntityOnLocalShards(address)) {
      return Effect.fail(new EntityNotAssignedToRunner({
        address
      }));
    }
    const state = entityManagers.get(address.entityType);
    if (!state) {
      return Effect.fail(new EntityNotManagedByRunner({
        address
      }));
    }
    return message._tag === "IncomingRequest" || message._tag === "IncomingEnvelope" ? state.manager.send(message) : runners.sendLocal({
      message,
      send: state.manager.sendLocal,
      simulateRemoteSerialization: config.simulateRemoteSerialization
    });
  });
  const notifyLocal = (message, discard) => Effect.suspend(() => {
    const address = message.envelope.address;
    if (!entityManagers.has(address.entityType)) {
      return Effect.fail(new EntityNotManagedByRunner({
        address
      }));
    }
    const isLocal = isEntityOnLocalShards(address);
    const notify = storageEnabled ? openStorageReadLatch : () => Effect.dieMessage("Sharding.notifyLocal: storage is disabled");
    if (message._tag === "IncomingRequest" || message._tag === "IncomingEnvelope") {
      if (message._tag === "IncomingRequest" && storageAlreadyProcessed(message)) {
        return Effect.fail(new AlreadyProcessingMessage({
          address,
          envelopeId: message.envelope.requestId
        }));
      } else if (!isLocal) {
        return Effect.fail(new EntityNotAssignedToRunner({
          address
        }));
      }
      return notify();
    }
    return runners.notifyLocal({
      message,
      notify,
      discard,
      storageOnly: !isLocal
    });
  });
  const isTransientError = Predicate.or(RunnerUnavailable.is, EntityNotAssignedToRunner.is);
  function sendOutgoing(message, discard, retries) {
    return Effect.catchIf(Effect.suspend(() => {
      const address = message.envelope.address;
      const maybeRunner = MutableHashMap.get(shardAssignments, address.shardId);
      const isPersisted = Context.get(message.rpc.annotations, Persisted);
      if (isPersisted && !storageEnabled) {
        return Effect.dieMessage("Sharding.sendOutgoing: Persisted messages require MessageStorage");
      }
      const runnerIsLocal = Option.isSome(maybeRunner) && isLocalRunner(maybeRunner.value);
      if (isPersisted) {
        return runnerIsLocal ? notifyLocal(message, discard) : runners.notify({
          address: maybeRunner,
          message,
          discard
        });
      } else if (Option.isNone(maybeRunner)) {
        return Effect.fail(new EntityNotAssignedToRunner({
          address
        }));
      }
      return runnerIsLocal ? sendLocal(message) : runners.send({
        address: maybeRunner.value,
        message
      });
    }), isTransientError, error => {
      if (retries === 0) {
        return Effect.die(error);
      }
      return Effect.delay(sendOutgoing(message, discard, retries && retries - 1), config.sendRetryInterval);
    });
  }
  const reset = Effect.fnUntraced(function* (requestId) {
    yield* storage.clearReplies(requestId);
    sentRequestIds.delete(requestId);
  }, Effect.matchCause({
    onSuccess: () => true,
    onFailure: () => false
  }));
  // --- Shard Manager sync ---
  const shardManagerTimeoutFiber = yield* FiberHandle.make().pipe(Scope.extend(shardingScope));
  const startShardManagerTimeout = FiberHandle.run(shardManagerTimeoutFiber, Effect.flatMap(Effect.sleep(config.shardManagerUnavailableTimeout), () => {
    MutableHashMap.clear(shardAssignments);
    return clearSelfShards;
  }), {
    onlyIfMissing: true
  });
  const stopShardManagerTimeout = FiberHandle.clear(shardManagerTimeoutFiber);
  // Every time the link to the shard manager is lost, we re-register the runner
  // and re-subscribe to sharding events
  yield* Effect.gen(function* () {
    yield* Effect.logDebug("Registering with shard manager");
    if (!isShutdown.current && Option.isSome(config.runnerAddress)) {
      const machineId = yield* shardManager.register(config.runnerAddress.value, config.shardGroups);
      yield* snowflakeGen.setMachineId(machineId);
    }
    yield* stopShardManagerTimeout;
    yield* Effect.logDebug("Subscribing to sharding events");
    const mailbox = yield* shardManager.shardingEvents(config.runnerAddress);
    const startedLatch = yield* Deferred.make();
    const eventsFiber = yield* Effect.gen(function* () {
      while (true) {
        const [events, done] = yield* mailbox.takeAll;
        if (done) return;
        for (const event of events) {
          yield* Effect.logDebug("Received sharding event", event);
          switch (event._tag) {
            case "StreamStarted":
              {
                yield* Deferred.done(startedLatch, Exit.void);
                break;
              }
            case "ShardsAssigned":
              {
                for (const shard of event.shards) {
                  MutableHashMap.set(shardAssignments, shard, event.address);
                }
                if (!MutableRef.get(isShutdown) && isLocalRunner(event.address)) {
                  for (const shardId of event.shards) {
                    if (MutableHashSet.has(selfShards, shardId)) continue;
                    MutableHashSet.add(selfShards, shardId);
                  }
                  yield* activeShardsLatch.open;
                }
                break;
              }
            case "ShardsUnassigned":
              {
                for (const shard of event.shards) {
                  MutableHashMap.remove(shardAssignments, shard);
                }
                if (isLocalRunner(event.address)) {
                  for (const shard of event.shards) {
                    MutableHashSet.remove(selfShards, shard);
                  }
                  yield* activeShardsLatch.open;
                }
                break;
              }
            case "RunnerUnregistered":
              {
                if (!isLocalRunner(event.address)) break;
                return yield* Effect.fail(new ClusterError.RunnerNotRegistered({
                  address: event.address
                }));
              }
          }
        }
      }
    }).pipe(Effect.intoDeferred(startedLatch), Effect.zipRight(Effect.dieMessage("Shard manager event stream down")), Effect.forkScoped);
    // Wait for the stream to be established
    yield* Deferred.await(startedLatch);
    // perform a full sync every config.refreshAssignmentsInterval
    const syncFiber = yield* syncAssignments.pipe(Effect.andThen(Effect.sleep(config.refreshAssignmentsInterval)), Effect.forever, Effect.forkScoped);
    return yield* Fiber.joinAll([eventsFiber, syncFiber]);
  }).pipe(Effect.scoped, Effect.catchAllCause(cause => Effect.logDebug(cause)), Effect.zipRight(startShardManagerTimeout), Effect.repeat(Schedule.exponential(1000).pipe(Schedule.union(Schedule.spaced(10_000)))), Effect.annotateLogs({
    package: "@effect/cluster",
    module: "Sharding",
    fiber: "ShardManager sync",
    runner: config.runnerAddress
  }), Effect.interruptible, Effect.forkIn(shardingScope));
  const syncAssignments = Effect.gen(function* () {
    const assignments = yield* shardManager.getAssignments;
    yield* Effect.logDebug("Received shard assignments", assignments);
    for (const [shardId, runner] of assignments) {
      if (Option.isNone(runner)) {
        MutableHashMap.remove(shardAssignments, shardId);
        MutableHashSet.remove(selfShards, shardId);
        continue;
      }
      MutableHashMap.set(shardAssignments, shardId, runner.value);
      if (!isLocalRunner(runner.value)) {
        MutableHashSet.remove(selfShards, shardId);
        continue;
      }
      if (MutableRef.get(isShutdown) || MutableHashSet.has(selfShards, shardId)) {
        continue;
      }
      MutableHashSet.add(selfShards, shardId);
    }
    yield* activeShardsLatch.open;
  });
  const clientRequests = new Map();
  const clients = yield* ResourceMap.make(Effect.fnUntraced(function* (entity) {
    const client = yield* RpcClient.makeNoSerialization(entity.protocol, {
      spanPrefix: `${entity.type}.client`,
      disableTracing: !Context.get(entity.protocol.annotations, ClusterSchema.ClientTracingEnabled),
      supportsAck: true,
      generateRequestId: () => RequestId(snowflakeGen.unsafeNext()),
      flatten: true,
      onFromClient(options) {
        const address = Context.unsafeGet(options.context, ClientAddressTag);
        switch (options.message._tag) {
          case "Request":
            {
              const fiber = Option.getOrThrow(Fiber.getCurrentFiber());
              const id = Snowflake.Snowflake(options.message.id);
              const rpc = entity.protocol.requests.get(options.message.tag);
              let respond;
              if (!options.discard) {
                const entry = {
                  rpc: rpc,
                  context: fiber.currentContext
                };
                clientRequests.set(id, entry);
                respond = makeClientRespond(entry, client.write);
              } else {
                respond = clientRespondDiscard;
              }
              return sendOutgoing(new Message.OutgoingRequest({
                envelope: Envelope.makeRequest({
                  requestId: id,
                  address,
                  tag: options.message.tag,
                  payload: options.message.payload,
                  headers: options.message.headers,
                  traceId: options.message.traceId,
                  spanId: options.message.spanId,
                  sampled: options.message.sampled
                }),
                lastReceivedReply: Option.none(),
                rpc,
                context: fiber.currentContext,
                respond
              }), options.discard);
            }
          case "Ack":
            {
              const requestId = Snowflake.Snowflake(options.message.requestId);
              const entry = clientRequests.get(requestId);
              if (!entry) return Effect.void;
              return sendOutgoing(new Message.OutgoingEnvelope({
                envelope: new Envelope.AckChunk({
                  id: snowflakeGen.unsafeNext(),
                  address,
                  requestId,
                  replyId: entry.lastChunkId
                }),
                rpc: entry.rpc
              }), false);
            }
          case "Interrupt":
            {
              const requestId = Snowflake.Snowflake(options.message.requestId);
              const entry = clientRequests.get(requestId);
              if (!entry) return Effect.void;
              clientRequests.delete(requestId);
              if (Context.get(entry.rpc.annotations, Uninterruptible)) {
                return Effect.void;
              }
              // for durable messages, we ignore interrupts on shutdown or as a
              // result of a shard being resassigned
              const isTransientInterrupt = MutableRef.get(isShutdown) || options.message.interruptors.some(id => internalInterruptors.has(id));
              if (isTransientInterrupt && Context.get(entry.rpc.annotations, Persisted)) {
                return Effect.void;
              }
              return Effect.ignore(sendOutgoing(new Message.OutgoingEnvelope({
                envelope: new Envelope.Interrupt({
                  id: snowflakeGen.unsafeNext(),
                  address,
                  requestId
                }),
                rpc: entry.rpc
              }), false, 3));
            }
        }
        return Effect.void;
      }
    });
    yield* Scope.addFinalizer(yield* Effect.scope, Effect.withFiberRuntime(fiber => {
      internalInterruptors.add(fiber.id());
      return Effect.void;
    }));
    return entityId => {
      const id = EntityId.make(entityId);
      const address = ClientAddressTag.context(EntityAddress.make({
        shardId: getShardId(id, entity.getShardGroup(entityId)),
        entityId: id,
        entityType: entity.type
      }));
      const clientFn = function (tag, payload, options) {
        const context = options?.context ? Context.merge(options.context, address) : address;
        return client.client(tag, payload, {
          ...options,
          context
        });
      };
      const proxyClient = {};
      return new Proxy(proxyClient, {
        has(_, p) {
          return entity.protocol.requests.has(p);
        },
        get(target, p) {
          if (p in target) {
            return target[p];
          } else if (!entity.protocol.requests.has(p)) {
            return undefined;
          }
          return target[p] = (payload, options) => clientFn(p, payload, options);
        }
      });
    };
  }));
  const makeClient = entity => clients.get(entity);
  const clientRespondDiscard = _reply => Effect.void;
  const makeClientRespond = (entry, write) => reply => {
    switch (reply._tag) {
      case "Chunk":
        {
          entry.lastChunkId = reply.id;
          return write({
            _tag: "Chunk",
            clientId: 0,
            requestId: RequestId(reply.requestId),
            values: reply.values
          });
        }
      case "WithExit":
        {
          clientRequests.delete(reply.requestId);
          return write({
            _tag: "Exit",
            clientId: 0,
            requestId: RequestId(reply.requestId),
            exit: reply.exit
          });
        }
    }
  };
  // --- Entities ---
  const context = yield* Effect.context();
  const reaper = yield* EntityReaper;
  const registerEntity = Effect.fnUntraced(function* (entity, build, options) {
    if (Option.isNone(config.runnerAddress) || entityManagers.has(entity.type)) return;
    const scope = yield* Scope.make();
    const manager = yield* EntityManager.make(entity, build, {
      ...options,
      storage,
      runnerAddress: config.runnerAddress.value,
      sharding
    }).pipe(Effect.provide(context.pipe(Context.add(EntityReaper, reaper), Context.add(Scope.Scope, scope), Context.add(Snowflake.Generator, snowflakeGen))));
    entityManagers.set(entity.type, {
      entity,
      scope,
      manager
    });
    yield* PubSub.publish(events, EntityRegistered({
      entity
    }));
  });
  yield* Scope.addFinalizerExit(shardingScope, exit => Effect.forEach(entityManagers.values(), state => Effect.catchAllCause(Scope.close(state.scope, exit), cause => Effect.annotateLogs(Effect.logError("Error closing entity manager", cause), {
    entity: state.entity.type
  })), {
    concurrency: "unbounded",
    discard: true
  }));
  // --- Finalization ---
  if (Option.isSome(config.runnerAddress)) {
    const selfAddress = config.runnerAddress.value;
    // Unregister runner from shard manager when scope is closed
    yield* Scope.addFinalizer(shardingScope, Effect.gen(function* () {
      yield* Effect.logDebug("Unregistering runner from shard manager", selfAddress);
      yield* shardManager.unregister(selfAddress).pipe(Effect.catchAllCause(cause => Effect.logError("Error calling unregister with shard manager", cause)));
      yield* clearSelfShards;
    }));
  }
  yield* Scope.addFinalizer(shardingScope, Effect.withFiberRuntime(fiber => {
    MutableRef.set(isShutdown, true);
    internalInterruptors.add(fiber.id());
    return Effect.void;
  }));
  const activeEntityCount = Effect.gen(function* () {
    let count = 0;
    for (const state of entityManagers.values()) {
      count += yield* state.manager.activeEntityCount;
    }
    return count;
  });
  const sharding = Sharding.of({
    getRegistrationEvents,
    getShardId,
    isShutdown: Effect.sync(() => MutableRef.get(isShutdown)),
    registerEntity,
    registerSingleton,
    makeClient,
    send: sendLocal,
    sendOutgoing: (message, discard) => sendOutgoing(message, discard),
    notify: message => notifyLocal(message, false),
    activeEntityCount,
    pollStorage: storageReadLatch.open,
    reset
  });
  return sharding;
});
/**
 * @since 1.0.0
 * @category layers
 */
export const layer = /*#__PURE__*/Layer.scoped(Sharding, make).pipe(/*#__PURE__*/Layer.provide([Snowflake.layerGenerator, EntityReaper.Default]));
// Utilities
const ClientAddressTag = /*#__PURE__*/Context.GenericTag("@effect/cluster/Sharding/ClientAddress");
//# sourceMappingURL=Sharding.js.map